# -*- coding: utf-8 -*-
"""credit card

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u0oAA_fK8tGjGKRJUppxGKaUnBBT8DjN

# Importing a library that is not in Colaboratory

To import a library that's not in Colaboratory by default, you can use `!pip install` or `!apt-get install`.
"""

!pip install matplotlib-venn

!apt-get -qq install -y libfluidsynth1

"""# Install 7zip reader [libarchive](https://pypi.python.org/pypi/libarchive)"""

# https://pypi.python.org/pypi/libarchive
!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive

"""# Install GraphViz & [PyDot](https://pypi.python.org/pypi/pydot)"""

# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install pydot
import pydot

!pip install pycaret
!pip install pycaret --force-reinstall
!pip install --upgrade scikit-learn
!pip install --upgrade imblearn

"""# Install [cartopy](http://scitools.org.uk/cartopy/docs/latest/)"""

!pip install cartopy
import cartopy

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import scipy
from sklearn.metrics import classification_report,accuracy_score
from sklearn.ensemble import IsolationForest

from sklearn.svm import OneClassSVM
from google.colab import drive

df= pd.read_csv("/content/creditcard.csv")
df.head()

df.shape
df.isnull().sum()
fraud_check = pd.value_counts(df['Class'], sort = True)
fraud_check.plot(kind = 'bar', rot=0, color= 'r')
plt.title("Normal and Fraud Distribution")
plt.xlabel("Class")
plt.ylabel("Frequency")
plt.xticks(range(2))
plt.show()

fraud_people = df[df['Class']==1]
normal_people= df[df['Class']==0]

fraud_people.shape
normal_people.shape

fraud_people['Amount'].describe()
fraud_people['Amount'].describe()

graph, (plot1, plot2) = plt.subplots(2,1,sharex= True)
graph.suptitle('Average amount per class')
bins = 70

plot1.hist(fraud_people['Amount'] , bins = bins)
plot1.set_title('Fraud Amount')

plot2.hist(normal_people['Amount'] , bins = bins)
plot2.set_title('Normal Amount')

plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show();


plt.figure(figsize=(30,30))
g=sns.heatmap(df.corr(),annot=True)

columns = df.columns.tolist()
# Making our Independent Features
columns = [var for var in columns if var not in ["Class"]]
# Making our Dependent Variable
target = "Class"
x= df[columns]
y= df[target]

x.shape
y.shape
x.head()
y.head()


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)

# Impute missing values using SimpleImputer before fitting the model
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # Replace NaN with the mean of the column
x_train = imputer.fit_transform(x_train)
x_test = imputer.transform(x_test) # Also impute x_test for consistency

iso_forest= IsolationForest(n_estimators=100, max_samples=len(x_train),random_state=0, verbose=0)
iso_forest.fit(x_train,y_train)
ypred= iso_forest.predict(x_test)
ypred


ypred[ypred == 1] = 0
ypred[ypred == -1] = 1

print(accuracy_score(y_test,ypred))
print(classification_report(y_test,ypred))
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, ypred)

n_errors = (ypred != y_test).sum()
print("Isolation Forest have {} errors.".format(n_errors))
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')  # Replace NaN with the mean of the column
x_train_imputed = imputer.fit_transform(x_train)

# Now fit the model with the imputed data
svm = OneClassSVM(kernel='rbf', degree=3, gamma=0.1, nu=0.05, max_iter=-1)
svm.fit(x_train_imputed)

ypred1= svm.predict(x_test)
ypred1[ypred1 == 1] = 0
ypred1[ypred1 == -1] = 1

print(accuracy_score(y_test,ypred))
print(classification_report(y_test,ypred))
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, ypred)
n_errors = (ypred1 != y_test).sum()
print("SVM have {} errors.".format(n_errors))


df= pd.read_csv("creditcard.csv")
df.head()
from pycaret.classification import *
model= setup(data= df, target='Class')
compare_models()
random_forest= create_model('rf')
random_forest

tuned_model= tune_model('random_forest')
pred_holdout = predict_model(random_forest,data= x_test)
pred_holdout



from google.colab import drive
drive.mount('/content/drive')

# Install necessary libraries
!pip install pycaret

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import scipy
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# Mount Google Drive to access the dataset
from google.colab import drive
drive.mount('/content/drive')

# Load dataset from Google Drive (Update the path)
df = pd.read_csv("/content/creditcard.csv")

# Display first 5 rows
df.head()

# Check dataset shape and missing values
print(df.shape)
print(df.isnull().sum())

# Visualizing fraud vs normal transactions
fraud_check = df['Class'].value_counts()
fraud_check.plot(kind='bar', rot=0, color='r')
plt.title("Normal and Fraud Distribution")
plt.xlabel("Class")
plt.ylabel("Frequency")
plt.xticks(range(2))
plt.show()

# Separate fraud and normal transactions
fraud_people = df[df['Class'] == 1]
normal_people = df[df['Class'] == 0]

print(f"Fraud cases: {fraud_people.shape}")
print(f"Normal cases: {normal_people.shape}")

# Histogram of fraud vs normal amounts
fig, (plot1, plot2) = plt.subplots(2, 1, sharex=True)
fig.suptitle('Transaction Amount Distribution')

plot1.hist(fraud_people['Amount'], bins=70, color='red')
plot1.set_title('Fraud Amount')

plot2.hist(normal_people['Amount'], bins=70, color='blue')
plot2.set_title('Normal Amount')

plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show()

# Heatmap to show correlation between features
plt.figure(figsize=(20, 20))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

# Define features and target variable
columns = [col for col in df.columns if col != "Class"]
x = df[columns]
y = df["Class"]

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42)

# Impute missing values
imputer = SimpleImputer(strategy='mean')
x_train = imputer.fit_transform(x_train)
x_test = imputer.transform(x_test)

# ----------------- ISOLATION FOREST ----------------- #
iso_forest = IsolationForest(n_estimators=100, max_samples=len(x_train), random_state=0, verbose=0)
iso_forest.fit(x_train)  # Remove y_train since Isolation Forest is unsupervised

# Predict anomalies
ypred = iso_forest.predict(x_test)

# Convert predictions: 1 → 0 (Normal), -1 → 1 (Fraud)
ypred[ypred == 1] = 0
ypred[ypred == -1] = 1

# Model Evaluation
print("Isolation Forest Accuracy:", accuracy_score(y_test, ypred))
print(classification_report(y_test, ypred))
print("Confusion Matrix:\n", confusion_matrix(y_test, ypred))
print("Isolation Forest Errors:", (ypred != y_test).sum())

# ----------------- ONE-CLASS SVM ----------------- #
svm = OneClassSVM(kernel='rbf', degree=3, gamma=0.1, nu=0.05, max_iter=-1)
svm.fit(x_train)  # Fit only on x_train

# Predict anomalies
ypred1 = svm.predict(x_test)

# Convert predictions: 1 → 0 (Normal), -1 → 1 (Fraud)
ypred1[ypred1 == 1] = 0
ypred1[ypred1 == -1] = 1

# Model Evaluation
print("SVM Accuracy:", accuracy_score(y_test, ypred1))
print(classification_report(y_test, ypred1))
print("Confusion Matrix:\n", confusion_matrix(y_test, ypred1))
print("SVM Errors:", (ypred1 != y_test).sum())

# ----------------- PYCARET CLASSIFICATION ----------------- #
from pycaret.classification import *

# Setup PyCaret environment
clf = setup(data=df, target='Class', silent=True, session_id=42)

# Compare models
best_model = compare_models()

# Train Random Forest
random_forest = create_model('rf')

# Tune Random Forest model
tuned_rf = tune_model(random_forest)

# Make predictions on the test set
pred_holdout = predict_model(tuned_rf, data=x_test)

# Display results
print(pred_holdout)

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import scipy
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from google.colab import drive


# ✅ Load the dataset (Ensure the correct path)
df = pd.read_csv("/content/creditcard.csv")  # Change path if needed
print(df.head())

# ✅ Check for missing values
print(df.isnull().sum())

# ✅ Visualize Fraud vs Normal distribution
fraud_check = df['Class'].value_counts()
fraud_check.plot(kind='bar', rot=0, color='r')
plt.title("Normal and Fraud Distribution")
plt.xlabel("Class")
plt.ylabel("Frequency")
plt.xticks(range(2), ["Normal", "Fraud"])
plt.show()

# ✅ Separate fraud and normal transactions
fraud_people = df[df['Class'] == 1]
normal_people = df[df['Class'] == 0]

# ✅ Summary statistics
print(fraud_people['Amount'].describe())
print(normal_people['Amount'].describe())

# ✅ Visualizing transaction amounts
fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 6))
fig.suptitle('Average amount per class')
bins = 70

ax1.hist(fraud_people['Amount'], bins=bins, color='red')
ax1.set_title('Fraud Amount')

ax2.hist(normal_people['Amount'], bins=bins, color='blue')
ax2.set_title('Normal Amount')

plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show()

# ✅ Correlation Heatmap
plt.figure(figsize=(20, 15))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# ✅ Define independent (X) and dependent (y) variables
columns = df.columns.tolist()
columns = [var for var in columns if var != "Class"]  # Exclude target
target = "Class"

X = df[columns]
y = df[target]

# ✅ Split the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# ✅ Handle missing values
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# ✅ Apply Isolation Forest (Unsupervised)
iso_forest = IsolationForest(n_estimators=100, random_state=0, verbose=0)
iso_forest.fit(X_train)  # ✅ Do not pass y_train
y_pred = iso_forest.predict(X_test)

# Convert predictions (-1 for anomaly) to (1 for fraud, 0 for normal)
y_pred = np.where(y_pred == 1, 0, 1)

# ✅ Evaluate Model
print("Isolation Forest Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print(f"Isolation Forest Errors: {(y_pred != y_test).sum()}")

# ✅ Apply OneClassSVM
svm = OneClassSVM(kernel='rbf', gamma=0.1, nu=0.05, max_iter=-1)
svm.fit(X_train)  # ✅ Do not pass y_train
y_pred_svm = svm.predict(X_test)

# Convert predictions (-1 for fraud) to (1 for fraud, 0 for normal)
y_pred_svm = np.where(y_pred_svm == 1, 0, 1)

# ✅ Evaluate Model
print("OneClassSVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print(f"SVM Errors: {(y_pred_svm != y_test).sum()}")

# ✅ Install & Use PyCaret
!pip install -q pycaret
from pycaret.classification import *

# ✅ Set up PyCaret
clf = setup(data=df, target='Class', silent=True, session_id=42)

# ✅ Compare multiple models
best_model = compare_models()

# ✅ Train a Random Forest model
random_forest = create_model('rf')

# ✅ Tune the model
tuned_rf = tune_model(random_forest)

# ✅ Predict on test data
pred_holdout = predict_model(tuned_rf, data=X_test)
print(pred_holdout.head())